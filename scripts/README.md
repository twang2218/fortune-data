# ğŸ¯ Fortune Cookie Data ETL Pipeline

[![Python 3.11](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/downloads/)
[![Type Hints](https://img.shields.io/badge/Type%20Hints-Pydantic-success)](https://pydantic-docs.helpmanual.io/)
[![Code Style](https://img.shields.io/badge/Code%20Style-Black-black)](https://github.com/psf/black)

> A powerful, modular data pipeline for collecting, evaluating, and storing high-quality fortune cookie content.

---

## ğŸ—ï¸ Architecture Overview

The system follows the classic ETL (Extract, Transform, Load) architecture, with each stage carefully designed to ensure data quality and processing efficiency.

### ğŸ“¥ Extract Stage

The extraction stage is built around a flexible crawler system that can collect data from multiple sources:

```
extract/
â”œâ”€â”€ ğŸŒ crawler.py      # Base crawler implementation
â”œâ”€â”€ ğŸ”„ extractor.py    # Generic extractor interface
â””â”€â”€ ğŸ“š gushiwen.py     # Specialized crawlers
```

**Core Components:**

ğŸ”¹ `Crawler`
- Common web scraping functionality
- Intelligent request management
- Robust error handling

ğŸ”¹ **Specialized Implementations**
- `MingJuCrawler`: Famous quotes collector
- `GuShiCrawler`: Poetry content crawler
- `ShiWenCrawler`: Literary works collector
- `GushiwenCrawler`: Main crawler orchestrator

### ğŸ”„ Transform Stage

The transformation stage implements a multi-step pipeline for data processing:

```
transform/
â”œâ”€â”€ ğŸ”„ transformer.py  # Base transformer interface
â”œâ”€â”€ â­ scorer.py      # Quality scoring system
â””â”€â”€ ğŸ¯ filter.py      # Content filtering implementations
```

**Key Features:**

ğŸ”¹ `Transformer`
- Foundation for all transformation operations
- Chain processing support
- Type safety guarantees

ğŸ”¹ `Scorer` System
- ğŸ“Š Overall quality assessment
- ğŸŒŸ Popularity analysis
- ğŸ­ Sentiment analysis
- ğŸ“ Content clarity evaluation

ğŸ”¹ **Filtering Strategies**
- `FilterByLength`: Length validation
- `FilterByScore`: Quality threshold
- `FilterByRank`: Ranking-based selection

### ğŸ’¾ Load Stage

The loading stage provides a flexible storage system:

```
load/
â”œâ”€â”€ ğŸ“¦ loader.py    # Generic loader interface
â”œâ”€â”€ ğŸ“„ jsonl.py     # JSONL file storage
â””â”€â”€ ğŸ—ƒï¸ cookiedb.py  # Specialized cookie database
```

**Core Features:**

ğŸ”¹ Generic Loading Interface
- Parameterized type design
- Extensible storage backends
- Data integrity guarantees

## ğŸ“Š Data Model

The system uses a hierarchical data model:

```python
class Cookie:
    title: str       # Title
    author: str      # Author
    content: str     # Content
    source: str      # Source
    link: str        # Link
    evaluations: Score  # Quality scores

class CookieJar:
    name: str              # Name
    link: str             # Link
    cookies: List[Cookie]  # Cookie list
    language: str         # Language
    tags: List[str]       # Tags
    format: str           # Format
```

## ğŸ”„ Processing Flow

```mermaid
graph LR
    A[Crawl Data] --> B[Score Quality]
    B --> C[Filter Content]
    C --> D[Store Data]
    style A fill:#f9f,stroke:#333
    style B fill:#bbf,stroke:#333
    style C fill:#bfb,stroke:#333
    style D fill:#fbb,stroke:#333
```

1. **ğŸ“¥ Data Collection**
   - Intelligent crawling
   - Structured parsing
   - Source tracking

2. **âš¡ Data Processing**
   - Quality scoring
   - Multi-stage filtering
   - Metadata enrichment

3. **ğŸ’¾ Data Storage**
   - Persistent storage
   - Incremental updates
   - Integrity validation

## ğŸš€ Usage Guide

### Complete ETL Process

```python
from scripts.main import main

# Launch full processing pipeline
main()
```

### Partial Processing (Transform & Load Only)

1. Configure tasks in `tasks.jsonl`
2. Pipeline will skip extraction phase

## âš™ï¸ Configuration

| Config File | Purpose |
|-------------|---------|
| `.env` | Environment variables |
| `tasks.jsonl` | Task definitions |
| `model_config/` | Model configurations |

## ğŸ¯ Quality Assurance

Built-in multi-layer quality assurance:

âœ… Source validation  
âœ… ML-powered quality scoring  
âœ… Multi-stage filtering  
âœ… Data integrity checks  

Ensures high standards in:
- ğŸ“š Content relevance
- ğŸ¯ Language quality
- ğŸŒŸ Cultural significance
- âš¡ Technical accuracy

---

<div align="center">

**ğŸŒŸ Building High-Quality Cultural Dataset ğŸŒŸ**

</div>
